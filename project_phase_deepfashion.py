# -*- coding: utf-8 -*-
"""project_phase_deepfashion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11mMElx3Wt7YpLSrclcWbY0udvntf-b1T
"""

!unzip /content/drive/MyDrive/dataset.zip

from google.colab import drive
drive.mount('/content/drive')

"""# **cGAN**"""

import os
import tensorflow as tf
from tensorflow.keras import layers, Model
from sklearn.preprocessing import LabelEncoder
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt

# Define the base path for the DeepFashion dataset
base_path = '/content/dataset'

def load_images_and_labels(base_path, target_size=(128, 128)):
    images = []
    labels = []
    for category_folder in os.listdir(base_path):
        category_path = os.path.join(base_path, category_folder)
        if os.path.isdir(category_path):
            for file in os.listdir(category_path):
                if file.lower().endswith('.jpg'):
                    img_path = os.path.join(category_path, file)
                    img = Image.open(img_path).convert('RGB')
                    img = img.resize(target_size)
                    images.append(np.array(img))
                    labels.append(category_folder)
    images = np.array(images, dtype='float32')
    images = (images - 127.5) / 127.5  # Normalize images to [-1, 1]
    return images, labels

# Load images and labels
images, labels = load_images_and_labels(base_path)

# Encode labels
label_encoder = LabelEncoder()
labels_encoded = label_encoder.fit_transform(labels)

!pip install tensorflow-addons

import tensorflow_addons as tfa
from tensorflow.keras import layers, Model

def residual_block(x, filters):
    """A simple residual block with two convolution layers"""
    shortcut = x
    x = layers.Conv2D(filters, (3, 3), padding='same', use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.LeakyReLU()(x)
    x = layers.Conv2D(filters, (3, 3), padding='same', use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.LeakyReLU()(x)
    x = layers.Add()([shortcut, x])  # Ensure addition is between identical shapes
    return x

def add_noise(x):
    noise = tf.random.normal(shape=tf.shape(x), mean=0.0, stddev=1.0)
    return x + noise

# def make_generator_model():
#     noise = layers.Input(shape=(100,))
#     label = layers.Input(shape=(1,), dtype='int32')

#     labels = layers.Embedding(len(label_encoder.classes_), 50)(label)
#     labels = layers.Flatten()(labels)

#     model_input = layers.concatenate([noise, labels])

#     x = layers.Dense(8*8*512, use_bias=False)(model_input)
#     x = layers.BatchNormalization()(x)
#     x = layers.LeakyReLU()(x)
#     x = layers.Reshape((8, 8, 512))(x)

#     x = add_noise(x)  # Inject noise
#     x = residual_block(x, 512)

#     # Correct number of Upsample and Conv2D layers to reach 128x128 resolution
#     x = layers.UpSampling2D()(x)  # 16x16
#     x = layers.Conv2D(256, (5, 5), padding='same', use_bias=False)(x)
#     x = layers.BatchNormalization()(x)
#     x = layers.LeakyReLU()(x)

#     x = layers.UpSampling2D()(x)  # 32x32
#     x = layers.Conv2D(128, (5, 5), padding='same', use_bias=False)(x)
#     x = layers.BatchNormalization()(x)
#     x = layers.LeakyReLU()(x)

#     x = layers.UpSampling2D()(x)  # 64x64
#     x = layers.Conv2D(64, (5, 5), padding='same', use_bias=False)(x)
#     x = layers.BatchNormalization()(x)
#     x = layers.LeakyReLU()(x)

#     x = layers.UpSampling2D()(x)  # 128x128
#     x = layers.Conv2D(3, (5, 5), padding='same', use_bias=False, activation='tanh')(x)

#     model = Model([noise, label], x)
#     return model
def make_generator_model():
    noise = layers.Input(shape=(100,))
    label = layers.Input(shape=(1,), dtype='int32')

    # Embedding for labels
    labels = layers.Embedding(len(label_encoder.classes_), 50)(label)
    labels = layers.Flatten()(labels)

    # Combine noise and label embeddings
    model_input = layers.concatenate([noise, labels])

    # Dense layer to upscale the combined input
    x = layers.Dense(8*8*256, use_bias=False)(model_input)
    x = layers.BatchNormalization()(x)
    x = layers.LeakyReLU()(x)
    x = layers.Reshape((8, 8, 256))(x)

    # Adding a residual block
    x = residual_block(x, 256)  # Using a residual block with 256 filters

    # Conv2DTranspose layers to scale up to 128x128
    x = layers.Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x)  # 16x16
    x = layers.BatchNormalization()(x)
    x = layers.LeakyReLU()(x)

    x = layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x)  # 32x32
    x = layers.BatchNormalization()(x)
    x = layers.LeakyReLU()(x)

    x = layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x)  # 64x64
    x = layers.BatchNormalization()(x)
    x = layers.LeakyReLU()(x)

    x = layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x)  # 128x128
    x = layers.BatchNormalization()(x)
    x = layers.LeakyReLU()(x)

    # Final Conv2DTranspose layer to maintain the output size and add the tanh activation
    x = layers.Conv2DTranspose(3, (5, 5), strides=(1, 1), padding='same', use_bias=False, activation='tanh')(x)  # 128x128

    model = Model([noise, label], x)
    return model

# def make_generator_model():
#     noise = layers.Input(shape=(100,))
#     label = layers.Input(shape=(1,), dtype='int32')

#     # Embedding for labels
#     labels = layers.Embedding(len(label_encoder.classes_), 50)(label)
#     labels = layers.Flatten()(labels)

#     # Combine noise and label embeddings
#     model_input = layers.concatenate([noise, labels])

#     # Dense layer to upscale the combined input
#     x = layers.Dense(8*8*256, use_bias=False)(model_input)
#     x = layers.BatchNormalization()(x)
#     x = layers.LeakyReLU()(x)
#     x = layers.Reshape((8, 8, 256))(x)

#     # Residual Block
#     x = residual_block(x, 256)  # Adding a residual block with the same number of filters

#     # Using convolutional transpose layers to upscale to the final image size
#     x = layers.Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x)
#     x = layers.BatchNormalization()(x)
#     x = layers.LeakyReLU()(x)

#     x = layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x)
#     x = layers.BatchNormalization()(x)
#     x = layers.LeakyReLU()(x)

#     x = layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x)
#     x = layers.BatchNormalization()(x)
#     x = layers.LeakyReLU()(x)

#     # Add another Conv2DTranspose layer to reach the correct output size
#     x = layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')(x)

#     model = Model([noise, label], x)
#     return model



def make_discriminator_model():
    image_input = layers.Input(shape=(128, 128, 3), name='image_input')
    label_input = layers.Input(shape=(1,), dtype='int32', name='label_input')

    labels = layers.Embedding(input_dim=len(label_encoder.classes_), output_dim=128 * 128)(label_input)
    labels = layers.Flatten()(labels)
    labels = layers.Reshape((128, 128, 1))(labels)

    # Concatenate image and labels
    concatenated = layers.Concatenate()([image_input, labels])

    x = tfa.layers.SpectralNormalization(
        layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same'))(concatenated)
    x = layers.LeakyReLU()(x)
    x = layers.Dropout(0.3)(x)

    x = tfa.layers.SpectralNormalization(
        layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))(x)
    x = layers.LeakyReLU()(x)
    x = layers.Dropout(0.3)(x)

    x = layers.Flatten()(x)
    x = layers.Dense(1)(x)

    model = Model(inputs=[image_input, label_input], outputs=x)
    return model

# def make_discriminator_model():
#     image = layers.Input(shape=(128, 128, 3))
#     label = layers.Input(shape=(1,), dtype='int32')
#     labels = layers.Embedding(len(label_encoder.classes_), 128 * 128)(label)
#     labels = layers.Flatten()(labels)
#     labels = layers.Reshape((128, 128, 1))(labels)
#     concatenated = layers.concatenate([image, labels])

#     x = tfa.layers.SpectralNormalization(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same'))(concatenated)
#     x = layers.LeakyReLU()(x)
#     x = layers.Dropout(0.3)(x)

#     x = tfa.layers.SpectralNormalization(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))(x)
#     x = layers.LeakyReLU()(x)
#     x = layers.Dropout(0.3)(x)

#     x = layers.Flatten()(x)
#     x = layers.Dense(1)(x)

#     model = Model([image, label], x)
#     return model


# def make_discriminator_model():
#     image = layers.Input(shape=(128, 128, 3))
#     label = layers.Input(shape=(1,), dtype='int32')
#     labels = layers.Embedding(len(label_encoder.classes_), 128 * 128)(label)
#     labels = layers.Flatten()(labels)
#     labels = layers.Reshape((128, 128, 1))(labels)
#     concatenated = layers.concatenate([image, labels])
#     x = layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same')(concatenated)
#     x = layers.LeakyReLU()(x)
#     x = layers.Dropout(0.3)(x)
#     x = layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')(x)
#     x = layers.LeakyReLU()(x)
#     x = layers.Dropout(0.3)(x)
#     x = layers.Flatten()(x)
#     x = layers.Dense(1)(x)
#     model = Model([image, label], x)
#     return model


# Define the generator model
# def make_generator_model():
#     noise = layers.Input(shape=(100,))
#     label = layers.Input(shape=(1,), dtype='int32')
#     labels = layers.Embedding(len(label_encoder.classes_), 50)(label)
#     labels = layers.Flatten()(labels)

#     model_input = layers.concatenate([noise, labels])

#     x = layers.Dense(8*8*256, use_bias=False)(model_input)
#     x = layers.BatchNormalization()(x)
#     x = layers.LeakyReLU()(x)
#     x = layers.Reshape((8, 8, 256))(x)

#     x = layers.Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x)
#     x = layers.BatchNormalization()(x)
#     x = layers.LeakyReLU()(x)

#     x = layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x)
#     x = layers.BatchNormalization()(x)
#     x = layers.LeakyReLU()(x)

#     x = layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x)
#     x = layers.BatchNormalization()(x)
#     x = layers.LeakyReLU()(x)

#     x = layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')(x)

#     model = Model([noise, label], x)
#     return model

# # Define the discriminator model
# def make_discriminator_model():
#     image = layers.Input(shape=(128, 128, 3))
#     label = layers.Input(shape=(1,), dtype='int32')
#     labels = layers.Embedding(len(label_encoder.classes_), 128 * 128)(label)
#     labels = layers.Flatten()(labels)
#     labels = layers.Reshape((128, 128, 1))(labels)

#     concatenated = layers.concatenate([image, labels])

#     x = layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same')(concatenated)
#     x = layers.LeakyReLU()(x)
#     x = layers.Dropout(0.3)(x)

#     x = layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')(x)
#     x = layers.LeakyReLU()(x)
#     x = layers.Dropout(0.3)(x)

#     x = layers.Flatten()(x)
#     x = layers.Dense(1)(x)

#     model = Model([image, label], x)
#     return model

# Define the loss functions
def generator_loss(fake_output):
    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_output, labels=tf.ones_like(fake_output)))

def discriminator_loss(real_output, fake_output):
    real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real_output, labels=tf.ones_like(real_output)))
    fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_output, labels=tf.zeros_like(fake_output)))
    total_loss = real_loss + fake_loss
    return total_loss

# Define the optimizer
generator_optimizer = tf.keras.optimizers.Adam(1e-4)
discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)

# Define the generator and discriminator models
generator = make_generator_model()
discriminator = make_discriminator_model()

# Define the training step
def train(dataset, epochs, report_frequency=10):
    for epoch in range(epochs):
        gen_loss_list = []
        disc_loss_list = []

        for image_batch, label_batch in dataset:
            gen_loss, disc_loss = train_step(image_batch, label_batch)
            gen_loss_list.append(gen_loss.numpy())
            disc_loss_list.append(disc_loss.numpy())

        # Reporting
        if (epoch + 1) % report_frequency == 0 or epoch == 0:
            gen_loss_avg = np.mean(gen_loss_list)
            disc_loss_avg = np.mean(disc_loss_list)
            print(f'Epoch {epoch + 1}, Generator Loss: {gen_loss_avg}, Discriminator Loss: {disc_loss_avg}')

@tf.function
# def train_step(images, labels):
#     batch_size = tf.shape(images)[0]
#     noise = tf.random.normal([batch_size, 100])

#     with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
#         generated_images = generator([noise, labels], training=True)
#         real_output = discriminator([images, labels], training=True)
#         fake_output = discriminator([generated_images, labels], training=True)

#         gen_loss = generator_loss(fake_output)
#         disc_loss = discriminator_loss(real_output, fake_output)

#     gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
#     gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

#     generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
#     discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

#     return gen_loss, disc_loss

@tf.function
def train_step(images, labels):
    batch_size = tf.shape(images)[0]
    noise = tf.random.normal([batch_size, 100])

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        generated_images = generator([noise, labels], training=True)

        real_output = discriminator([images, labels], training=True)
        fake_output = discriminator([generated_images, labels], training=True)

        gen_loss = generator_loss(fake_output)
        disc_loss = discriminator_loss(real_output, fake_output)

    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

    return gen_loss, disc_loss

# Define the dataset creation function
def create_dataset(images, labels, batch_size):
    # Convert the images and labels to TensorFlow tensors
    images = tf.convert_to_tensor(images, dtype=tf.float32)
    labels = tf.convert_to_tensor(labels, dtype=tf.int32)

    # Create a TensorFlow dataset from tensors
    dataset = tf.data.Dataset.from_tensor_slices((images, labels))

    # Shuffle and batch the dataset
    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(tf.data.AUTOTUNE)
    return dataset

# Define the training function
def train(dataset, epochs, report_frequency=10):
    gen_losses = []
    disc_losses = []

    for epoch in range(epochs):
        gen_loss_list = []
        disc_loss_list = []

        for image_batch, label_batch in dataset:
            gen_loss, disc_loss = train_step(image_batch, label_batch)
            gen_loss_list.append(gen_loss.numpy())
            disc_loss_list.append(disc_loss.numpy())

        # Calculate the average losses for this epoch
        gen_loss_avg = np.mean(gen_loss_list)
        disc_loss_avg = np.mean(disc_loss_list)

        gen_losses.append(gen_loss_avg)
        disc_losses.append(disc_loss_avg)

        # Optional: Report losses periodically
        if (epoch + 1) % report_frequency == 0 or epoch == 0:
            print(f'Epoch {epoch + 1}, Generator Loss: {gen_loss_avg}, Discriminator Loss: {disc_loss_avg}')

    return gen_losses, disc_losses




# Define parameters
batch_size = 32  # You can adjust the batch size according to your GPU memory
num_epochs = 100 # Define the number of epochs

# Create dataset
dataset = create_dataset(images, labels_encoded, batch_size)

# Run the training
# Run the training with loss reporting
# train(dataset, num_epochs, report_frequency=1)  # Report every epoch
import matplotlib.pyplot as plt

# Assuming you have a function train() that returns gen_losses and disc_losses
gen_losses, disc_losses = train(dataset, num_epochs, report_frequency=10)

# Plotting the losses
plt.figure(figsize=(10, 5))
plt.plot(gen_losses, label='Generator Loss')
plt.plot(disc_losses, label='Discriminator Loss')
plt.title('Training Losses')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import matplotlib.pyplot as plt

# Define the function to generate and display images
def generate_and_display_images(generator, num_examples=10, rows=5, cols=2):
    noise = tf.random.normal([num_examples, 100])
    sampled_labels = np.random.randint(0, len(label_encoder.classes_), num_examples)
    sampled_labels = tf.convert_to_tensor(sampled_labels, dtype=tf.int32)

    predictions = generator([noise, sampled_labels], training=False)

    fig, axes = plt.subplots(rows, cols, figsize=(10, 10))
    for i, ax in enumerate(axes.flat):
        ax.imshow((predictions[i, :, :, :] + 1) / 2)  # Scale the images to the [0, 1] range
        ax.set_title(f'Label: {label_encoder.classes_[sampled_labels[i]]}')
        ax.axis('off')
    plt.tight_layout()
    plt.show()
# Generate and display images
generate_and_display_images(generator)

import tensorflow as tf
from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input
from scipy.linalg import sqrtm

import os
import numpy as np
from PIL import Image
import tensorflow as tf

# Define the base path for the dataset
base_path = '/content/dataset'

def load_images_and_labels(base_path, target_size=(128, 128), max_images_per_label=None):
    images = []
    labels = []
    label_counts = {}

    for category_folder in os.listdir(base_path):
        category_path = os.path.join(base_path, category_folder)
        if os.path.isdir(category_path):
            for file in os.listdir(category_path):
                if file.lower().endswith('.jpg'):
                    img_path = os.path.join(category_path, file)
                    img = Image.open(img_path).convert('RGB')
                    img = img.resize(target_size)

                    if max_images_per_label is not None:
                        if label_counts.get(category_folder, 0) >= max_images_per_label:
                            continue
                        label_counts[category_folder] = label_counts.get(category_folder, 0) + 1

                    images.append(np.array(img))
                    labels.append(category_folder)

    images = np.array(images, dtype='float32')
    images = (images - 127.5) / 127.5  # Normalize images to [-1, 1]
    return images, labels

# Load images and labels with a balanced set
images, labels = load_images_and_labels(base_path, max_images_per_label=100)  # Adjust as needed

# Encode labels
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
labels_encoded = label_encoder.fit_transform(labels)

# Convert images for FID computation
 # Convert back to [0, 255] range for FID calculation

# Now, `real_images` contains a balanced dataset that can be used for FID calculation


# Load InceptionV3 model
model = InceptionV3(include_top=False, pooling='avg', input_shape=(128, 128, 3))

def calculate_fid(model, real_images, generated_images):
    # Preprocess images
    real_images = preprocess_input(real_images)
    generated_images = preprocess_input(generated_images)

    # Calculate activations
    act1 = model.predict(real_images)
    act2 = model.predict(generated_images)

    # Calculate mean and covariance statistics
    mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)
    mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)

    # Calculate sum squared difference between means
    ssdiff = np.sum((mu1 - mu2)**2.0)

    # Calculate sqrt of product between cov
    covmean = sqrtm(sigma1.dot(sigma2))

    # Check and correct imaginary numbers from sqrt
    if np.iscomplexobj(covmean):
        covmean = covmean.real

    # Calculate score
    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)
    return fid

# Generate images for FID computation
def generate_images(generator, num_images):
    noise = tf.random.normal([num_images, 100])
    sampled_labels = np.random.randint(0, len(label_encoder.classes_), num_images)
    sampled_labels = tf.convert_to_tensor(sampled_labels, dtype=tf.int32)
    generated_images = generator([noise, sampled_labels], training=False)
    generated_images = (generated_images + 1) * 127.5  # Scale images to [0, 255]
    return generated_images.numpy()

# Example usage
num_images = 1000  # Number of images to generate for FID calculation
generated_images = generate_images(generator, num_images)
real_images = images * 127.5 + 127.5

fid_score = calculate_fid(model, real_images, generated_images)
print('FID score:', fid_score)

import os
import tensorflow as tf
from tensorflow.keras import layers, Model
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt

# Define the base path for the DeepFashion dataset
base_path = '/content/dataset'

def load_images_and_labels(base_path, target_size=(128, 128)):
    images = []
    labels = []
    for category_folder in os.listdir(base_path):
        category_path = os.path.join(base_path, category_folder)
        if os.path.isdir(category_path):
            for file in os.listdir(category_path):
                if file.lower().endswith('.jpg'):
                    img_path = os.path.join(category_path, file)
                    img = Image.open(img_path).convert('RGB')
                    img = img.resize(target_size)
                    images.append(np.array(img))
                    labels.append(category_folder)
    images = np.array(images, dtype='float32')
    images = (images - 127.5) / 127.5  # Normalize images to [-1, 1]
    return images, labels

# Load images and labels
images, labels = load_images_and_labels(base_path)

# Encode labels
label_encoder = LabelEncoder()
labels_encoded = label_encoder.fit_transform(labels)

# Split the data into train and test sets
train_images, test_images, train_labels, test_labels = train_test_split(images, labels_encoded, test_size=0.2, random_state=42)

# Define the generator model
def make_generator_model():
    noise = layers.Input(shape=(100,))
    label = layers.Input(shape=(1,), dtype='int32')
    labels = layers.Embedding(len(label_encoder.classes_), 50)(label)
    labels = layers.Flatten()(labels)

    model_input = layers.concatenate([noise, labels])

    x = layers.Dense(8*8*256, use_bias=False)(model_input)
    x = layers.BatchNormalization()(x)
    x = layers.LeakyReLU()(x)
    x = layers.Reshape((8, 8, 256))(x)

    x = layers.Conv2DTranspose(256, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.LeakyReLU()(x)

    x = layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.LeakyReLU()(x)

    x = layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.LeakyReLU()(x)

    x = layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')(x)

    model = Model([noise, label], x)
    return model

# Define the discriminator model
def make_discriminator_model():
    image = layers.Input(shape=(128, 128, 3))
    label = layers.Input(shape=(1,), dtype='int32')
    labels = layers.Embedding(len(label_encoder.classes_), 128 * 128)(label)
    labels = layers.Flatten()(labels)
    labels = layers.Reshape((128, 128, 1))(labels)

    concatenated = layers.concatenate([image, labels])

    x = layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same')(concatenated)
    x = layers.LeakyReLU()(x)
    x = layers.Dropout(0.3)(x)

    x = layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')(x)
    x = layers.LeakyReLU()(x)
    x = layers.Dropout(0.3)(x)

    x = layers.Flatten()(x)
    x = layers.Dense(1)(x)

    model = Model([image, label], x)
    return model

# Define the loss functions
def generator_loss(fake_output):
    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_output, labels=tf.ones_like(fake_output)))

def discriminator_loss(real_output, fake_output):
    real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real_output, labels=tf.ones_like(real_output)))
    fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_output, labels=tf.zeros_like(fake_output)))
    total_loss = real_loss + fake_loss
    return total_loss

# Define the optimizer
generator_optimizer = tf.keras.optimizers.Adam(1e-4)
discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)

# Define the generator and discriminator models
generator = make_generator_model()
discriminator = make_discriminator_model()

# Define the training step
@tf.function
def train_step(images, labels):
    # Generating noise from a normal distribution
    batch_size = tf.shape(images)[0]
    noise = tf.random.normal([batch_size, 100])

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        # Generator expects a list of inputs: [noise, labels]
        generated_images = generator([noise, labels], training=True)

        # Discriminator gets the real images along with their labels and the generated images with their labels
        real_output = discriminator([images, labels], training=True)
        fake_output = discriminator([generated_images, labels], training=True)

        gen_loss = generator_loss(fake_output)
        disc_loss = discriminator_loss(real_output, fake_output)

    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

    return gen_loss, disc_loss


# Define the dataset creation function
def create_dataset(images, labels, batch_size):
    # Convert the images and labels to TensorFlow tensors
    images = tf.convert_to_tensor(images, dtype=tf.float32)
    labels = tf.convert_to_tensor(labels, dtype=tf.int32)

    # Create a TensorFlow dataset from tensors
    dataset = tf.data.Dataset.from_tensor_slices((images, labels))

    # Shuffle and batch the dataset
    dataset = dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(tf.data.AUTOTUNE)
    return dataset

# Define the training function with loss monitoring
def train(train_dataset, test_dataset, epochs, early_stopping_patience=5):
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    patience_count = 0

    for epoch in range(epochs):
        epoch_gen_loss_avg = tf.keras.metrics.Mean()
        epoch_disc_loss_avg = tf.keras.metrics.Mean()

        for image_batch, label_batch in train_dataset:
            gen_loss, disc_loss = train_step(image_batch, label_batch)
            epoch_gen_loss_avg.update_state(gen_loss)
            epoch_disc_loss_avg.update_state(disc_loss)

        train_loss = epoch_disc_loss_avg.result()
        val_loss = evaluate_model(generator, test_dataset)
        print(f'Epoch {epoch + 1}, Train Loss: {train_loss}, Val Loss: {val_loss}')

        train_losses.append(train_loss)
        val_losses.append(val_loss)

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_count = 0
        else:
            patience_count += 1

        if patience_count >= early_stopping_patience:
            print(f'Early stopping at epoch {epoch + 1}')
            break

    return train_losses, val_losses

# Evaluate the model on the test set
def evaluate_model(generator, test_dataset):
    total_loss = 0
    num_batches = 0
    for image_batch, label_batch in test_dataset:
        noise = tf.random.normal([image_batch.shape[0], 100])
        sampled_labels = np.random.randint(0, len(label_encoder.classes_), image_batch.shape[0])
        sampled_labels = tf.convert_to_tensor(sampled_labels, dtype=tf.int32)

        generated_images = generator([noise, sampled_labels], training=False)
        fake_output = discriminator([generated_images, sampled_labels], training=False)

        total_loss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_output, labels=tf.zeros_like(fake_output)))
        num_batches += 1

    return total_loss / num_batches

# Train and evaluate the models
batch_size = 32
num_epochs = 50
train_dataset = create_dataset(train_images, train_labels, batch_size)
test_dataset = create_dataset(test_images, test_labels, batch_size)

train_losses, val_losses = train(train_dataset, test_dataset, num_epochs)

# Plot training and validation losses
plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')
plt.plot(range(1, len(val_losses) + 1), val_losses, label='Val Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Losses')
plt.legend()
plt.show()

# Save the trained models
generator.save('generator_model.h5')
discriminator.save('discriminator_model.h5')

"""# **DCGANs**"""

import tensorflow as tf
import matplotlib.pyplot as plt
from scipy import linalg
from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.applications.inception_v3 import preprocess_input

def load_and_preprocess_image(path):
    image = tf.io.read_file(path)
    image = tf.image.decode_jpeg(image, channels=3)
    image = tf.image.resize(image, [128, 128])  # Resize images
    image = (image - 127.5) / 127.5  # Normalize images to [-1, 1]
    return image

def load_dataset(directory, batch_size):
    dataset = tf.data.Dataset.list_files(directory + '/*/*.jpg')
    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.shuffle(buffer_size=1000).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)
    return dataset

data_directory = '/content/dataset'
batch_size = 32
dataset = load_dataset(data_directory, batch_size)

def make_generator_model():
    model = tf.keras.Sequential([
        # Start with a fully connected layer that receives a 100-dimensional noise vector as input
        tf.keras.layers.Dense(16*16*256, use_bias=False, input_shape=(100,)),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.LeakyReLU(),
        # Reshape to a 16x16x256 tensor
        tf.keras.layers.Reshape((16, 16, 256)),
        # Transposed convolution, size becomes 16x16 -> 32x32
        tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.LeakyReLU(),
        # Transposed convolution, size becomes 32x32 -> 64x64
        tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.LeakyReLU(),
        # Transposed convolution, final size becomes 64x64 -> 128x128
        tf.keras.layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')
    ])
    return model


def make_discriminator_model():
    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[128, 128, 3]),
        tf.keras.layers.LeakyReLU(),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),
        tf.keras.layers.LeakyReLU(),
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(1)
    ])
    return model

generator = make_generator_model()
discriminator = make_discriminator_model()

generator_optimizer = tf.keras.optimizers.Adam(1e-4)
discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)

cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

def generator_loss(fake_output):
    return cross_entropy(tf.ones_like(fake_output), fake_output)

def discriminator_loss(real_output, fake_output):
    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    return real_loss + fake_loss

@tf.function
def train_step(images, generator, discriminator, generator_optimizer, discriminator_optimizer):
    noise = tf.random.normal([batch_size, 100])

    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        generated_images = generator(noise, training=True)

        real_output = discriminator(images, training=True)
        fake_output = discriminator(generated_images, training=True)

        gen_loss = generator_loss(fake_output)
        disc_loss = discriminator_loss(real_output, fake_output)

    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

    return gen_loss, disc_loss

# Function to train the GAN and monitor training loss
def train_and_monitor_loss(dataset, generator, discriminator, generator_optimizer, discriminator_optimizer, epochs):
    gen_losses = []
    disc_losses = []

    for epoch in range(epochs):
        epoch_gen_loss = []
        epoch_disc_loss = []

        for image_batch in dataset:
            gen_loss, disc_loss = train_step(image_batch, generator, discriminator, generator_optimizer, discriminator_optimizer)
            epoch_gen_loss.append(gen_loss)
            epoch_disc_loss.append(disc_loss)

        # Calculate average losses for the epoch
        average_gen_loss = sum(epoch_gen_loss) / len(epoch_gen_loss)
        average_disc_loss = sum(epoch_disc_loss) / len(epoch_disc_loss)

        # Store average losses
        gen_losses.append(average_gen_loss)
        disc_losses.append(average_disc_loss)

        # Print training progress
        print(f'Epoch {epoch + 1}, Generator Loss: {average_gen_loss}, Discriminator Loss: {average_disc_loss}')

    # Plot loss over epochs
    plt.plot(range(1, epochs + 1), gen_losses, label='Generator Loss')
    plt.plot(range(1, epochs + 1), disc_losses, label='Discriminator Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Training Loss')
    plt.legend()
    plt.show()

# Define the number of epochs
epochs = 100

# Train the GAN and monitor loss
train_and_monitor_loss(dataset, generator, discriminator, generator_optimizer, discriminator_optimizer, epochs)

def generate_and_display_images(model, num_examples=16):
    noise = tf.random.normal([num_examples, 100])
    generated_images = model(noise, training=False)

    fig = plt.figure(figsize=(8, 8))

    for i in range(generated_images.shape[0]):
        plt.subplot(4, 4, i+1)
        plt.imshow(generated_images[i, :, :, :] * 0.5 + 0.5)
        plt.axis('off')

    plt.show()

generate_and_display_images(generator)

import numpy as np
import tensorflow as tf
from tensorflow.keras.applications.inception_v3 import InceptionV3
from scipy.linalg import sqrtm

# Function to preprocess images for InceptionV3 model
def preprocess_images(images):
    processed_images = (images + 1) * 127.5
    processed_images = tf.keras.applications.inception_v3.preprocess_input(processed_images)
    return processed_images

# Function to compute InceptionV3 embeddings
def compute_embeddings(images):
    inception_model = InceptionV3(include_top=False, pooling='avg')
    embeddings = inception_model.predict(images)
    return embeddings

# Function to calculate FID score
def calculate_fid_score(real_images, generated_images):
    real_embeddings = compute_embeddings(real_images)
    generated_embeddings = compute_embeddings(generated_images)

    # Compute mean and covariance for real and generated embeddings
    mu_real, sigma_real = np.mean(real_embeddings, axis=0), np.cov(real_embeddings, rowvar=False)
    mu_generated, sigma_generated = np.mean(generated_embeddings, axis=0), np.cov(generated_embeddings, rowvar=False)

    # Compute squared Frobenius norm between means
    mean_diff_squared = np.sum((mu_real - mu_generated)**2)

    # Compute trace of the product of covariances
    sigma_sqrt = sqrtm(sigma_real @ sigma_generated)
    if np.iscomplexobj(sigma_sqrt):
        sigma_sqrt = sigma_sqrt.real
    trace_sigma = np.trace(sigma_real + sigma_generated - 2 * sigma_sqrt)

    # Compute FID score
    fid_score = mean_diff_squared + trace_sigma
    return fid_score

# Function to resize images to the required dimensions for InceptionV3
def resize_images(images):
    resized_images = tf.image.resize_with_pad(images, target_height=299, target_width=299)
    return resized_images

# Generate a set of images using the trained generator
num_generated_images = 1000
generated_images = generator(tf.random.normal([num_generated_images, 100]), training=False).numpy()

# Preprocess generated images
processed_generated_images = preprocess_images(generated_images)
processed_generated_images = resize_images(processed_generated_images)

# Load a set of real images (from the same distribution) and preprocess them
real_images = next(iter(dataset)).numpy()
processed_real_images = preprocess_images(real_images)
processed_real_images = resize_images(processed_real_images)

# Calculate FID score
fid_score = calculate_fid_score(processed_real_images, processed_generated_images)
print("FID Score:", fid_score)

"""# **GAN**

# **data loading and sampling**
"""

import os
import numpy as np
import tensorflow as tf

def load_and_preprocess_image(path):
    image = tf.io.read_file(path)
    image = tf.image.decode_jpeg(image, channels=3)
    image = tf.image.resize(image, [64, 64])  # Ensure this is set correctly
    image = (image - 127.5) / 127.5  # Normalize the image to [-1, 1]
    return image

def load_dataset(directory):
    file_paths = tf.data.Dataset.list_files(directory + '/*/*.jpg')
    dataset = file_paths.map(lambda x: (load_and_preprocess_image(x), tf.strings.split(x, os.path.sep)[-2]), num_parallel_calls=tf.data.AUTOTUNE)
    return dataset

data_directory = '/content/dataset'
dataset = load_dataset(data_directory)

# Splitting the dataset into training and testing sets
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size

train_dataset = dataset.take(train_size)
test_dataset = dataset.skip(train_size)

print("Train dataset size:", train_size)
print("Test dataset size:", test_size)

"""# **data visualization**"""

import matplotlib.pyplot as plt

# Visualize one sample for each class
def visualize_samples(dataset):
    class_names = os.listdir(data_directory)
    fig, axs = plt.subplots(1, len(class_names), figsize=(15, 3))
    for i, class_name in enumerate(class_names):
        for image, label in dataset:
            if label.numpy().decode("utf-8") == class_name:
                axs[i].imshow(image.numpy())
                axs[i].set_title(class_name)
                axs[i].axis('off')
                break
    plt.show()

# Visualize data distribution for each class using bar chart
def visualize_data_distribution(dataset):
    class_names = os.listdir(data_directory)
    class_counts = {class_name: 0 for class_name in class_names}
    for _, label in dataset:
        class_name = label.numpy().decode("utf-8")
        class_counts[class_name] += 1
    plt.bar(class_counts.keys(), class_counts.values())
    plt.xlabel('Class')
    plt.ylabel('Count')
    plt.title('Data Distribution')
    plt.show()

visualize_samples(train_dataset)
visualize_data_distribution(train_dataset)

import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential

def load_and_preprocess_image(path):
    image = tf.io.read_file(path)
    image = tf.image.decode_jpeg(image, channels=3)
    image = tf.image.resize(image, [64, 64])  # Ensure this is set correctly
    image = (image - 127.5) / 127.5  # Normalize the image to [-1, 1]
    return image

def load_dataset(directory, batch_size):
    dataset = tf.data.Dataset.list_files(directory + '/*/*.jpg')
    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.shuffle(buffer_size=1000).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)
    return dataset

# Set the path to your dataset and batch size
data_directory = '/content/dataset'
batch_size = 32
dataset = load_dataset(data_directory, batch_size)

def build_discriminator():
    model = Sequential([
        layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[64, 64, 3]),
        layers.LeakyReLU(),
        layers.Dropout(0.3),
        layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'),
        layers.LeakyReLU(),
        layers.Dropout(0.3),
        layers.Flatten(),
        layers.Dense(1)
    ])
    return model

def build_generator():
    model = Sequential([
        layers.Dense(8*8*256, use_bias=False, input_shape=(100,)),
        layers.BatchNormalization(),
        layers.LeakyReLU(),
        layers.Reshape((8, 8, 256)),
        layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False),
        layers.BatchNormalization(),
        layers.LeakyReLU(),
        layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False),
        layers.BatchNormalization(),
        layers.LeakyReLU(),
        layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')
    ])
    return model


generator = build_generator()
discriminator = build_discriminator()

cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

def discriminator_loss(real_output, fake_output):
    real_loss = cross_entropy(tf.ones_like(real_output), real_output)
    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
    return real_loss + fake_loss

def generator_loss(fake_output):
    return cross_entropy(tf.ones_like(fake_output), fake_output)

generator_optimizer = tf.keras.optimizers.Adam(1e-4)
discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)

@tf.function
def train_step(images):
    noise = tf.random.normal([batch_size, 100])
    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
        generated_images = generator(noise, training=True)
        real_output = discriminator(images, training=True)
        fake_output = discriminator(generated_images, training=True)

        gen_loss = generator_loss(fake_output)
        disc_loss = discriminator_loss(real_output, fake_output)

    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

# def train(dataset, epochs):
#     for epoch in range(epochs):
#         for image_batch in dataset:
#             train_step(image_batch)

# train(dataset, epochs=50)

import matplotlib.pyplot as plt
import tensorflow as tf

def train(dataset, epochs):
    generator_losses = []
    discriminator_losses = []

    for epoch in range(epochs):
        for image_batch in dataset:
            train_step(image_batch)

        # Generate noise and images for loss calculations
        noise = tf.random.normal([batch_size, 100])
        generated_images = generator(noise, training=False)
        real_output = discriminator(image_batch, training=False)
        fake_output = discriminator(generated_images, training=False)

        # Calculate losses
        gen_loss = generator_loss(fake_output)
        disc_loss = discriminator_loss(real_output, fake_output)

        # Store losses
        generator_losses.append(gen_loss.numpy())
        discriminator_losses.append(disc_loss.numpy())

        # Print the losses
        print("Epoch {}/{}: Generator Loss: {:.4f}, Discriminator Loss: {:.4f}".format(epoch+1, epochs, gen_loss, disc_loss))

    # Plot losses
    plt.figure(figsize=(10, 5))
    plt.plot(generator_losses, label='Generator Loss')
    plt.plot(discriminator_losses, label='Discriminator Loss')
    plt.title('Training Losses')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

# Example of how to call the train function
train(dataset, epochs=50)

import matplotlib.pyplot as plt
import tensorflow as tf

def generate_and_display_images(generator, num_images):
    noise = tf.random.normal([num_images, 100])
    generated_images = generator(noise, training=False)

    plt.figure(figsize=(10, 1))
    for i in range(num_images):
        plt.subplot(1, num_images, i+1)
        plt.imshow((generated_images[i] + 1) / 2)  # Scale images from [-1, 1] to [0, 1]
        plt.axis('off')
    plt.tight_layout()
    plt.show()

# Assuming you already have a trained generator model named 'generator'
num_images_to_generate = 10
generate_and_display_images(generator, num_images_to_generate)

import numpy as np
import tensorflow as tf
from scipy.linalg import sqrtm
from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.applications.inception_v3 import preprocess_input

def calculate_fid_score(generator, dataset, num_images=1000):
    # Load pre-trained InceptionV3 model
    inception_model = InceptionV3(include_top=False, pooling='avg', input_shape=(299, 299, 3))

    # Generate fake images
    noise = tf.random.normal([num_images, 100])
    generated_images = generator(noise, training=False).numpy()

    # Resize generated images to 299x299x3 for InceptionV3 input
    generated_images_resized = tf.image.resize(generated_images, (299, 299), method='bilinear')

    # Preprocess images for InceptionV3
    generated_images_preprocessed = preprocess_input(generated_images_resized)

    # Extract features for generated images
    generated_features = inception_model.predict(generated_images_preprocessed)

    # Extract features for real images
    real_features = []
    for image_batch in dataset.take(num_images):
        image_batch_resized = tf.image.resize(image_batch, (299, 299), method='bilinear')
        image_batch_preprocessed = preprocess_input(image_batch_resized)
        features = inception_model.predict(image_batch_preprocessed)
        real_features.append(features)
    real_features = np.concatenate(real_features, axis=0)

    # Calculate mean and covariance for real and generated features
    mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)
    mu2, sigma2 = generated_features.mean(axis=0), np.cov(generated_features, rowvar=False)

    # Calculate FID score
    diff = mu1 - mu2
    cov_mean, _ = sqrtm(sigma1.dot(sigma2), disp=False)
    if np.iscomplexobj(cov_mean):
        cov_mean = cov_mean.real
    fid_score = np.sum(diff**2) + np.trace(sigma1 + sigma2 - 2*cov_mean)

    return fid_score

# Example of how to use the function
fid_score = calculate_fid_score(generator, dataset)
print("FID Score:", fid_score)